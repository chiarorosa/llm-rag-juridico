{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ler documento\n",
    "\n",
    "Vamos usar um leitor de PDF simples. Certamente ele não vai lidar com tabelas, imagens, equações etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Figure 1: Example prompts for QA (left) and attributed QA (right, following (5)).',\n",
       " '3.2 Datasets',\n",
       " 'ASQA is a long-form QA dataset for factoid questions designed to evaluate a model’s performance',\n",
       " 'on naturally-occurring ambiguous questions ( 23). It is made up of 948 queries and the ground truth',\n",
       " 'documents are based on a 12/20/2018 Wikipedia dump with 21M passages. We use the set of five']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"artigo_exemplo.pdf\")\n",
    "    \n",
    "text = \"\"\n",
    "\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "text.split(\"\\n\")[100:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processar o documento\n",
    "\n",
    "Há muito o que ser feito para melhorar a qualidade da fonte de informação. Pode-se ver que o leitor de PDF quebra frases pela metade. Vamos pelo menos tentar juntá-las em sentenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Toward Optimal Search and Retrieval for RAG Alexandria Leto Department of Computer Science University of Colorado Boulder alex.leto@colorado.eduCecilia Aguerrebere Intel Labs cecilia.aguerrebere@intel.com Ishwar Bhati Intel Labs ishwar.s.bhati@intel.comTed Willke Intel Labs ted.willke@intel.com Mariano Tepper Intel Labs mariano.tepper@intel.comVy Ai Vo Intel Labs vy.vo@intel.com Abstract Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs).',\n",
       " 'Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood.',\n",
       " 'Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA).',\n",
       " 'We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines.',\n",
       " 'For example, lowering search ac- curacy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_lines(text):\n",
    "    merged = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if current_sentence:\n",
    "            line = ' ' + line\n",
    "            \n",
    "        current_sentence += line\n",
    "        \n",
    "        if \". \" in current_sentence:\n",
    "            parts = current_sentence.split(\". \")\n",
    "            for part in parts[:-1]:\n",
    "                merged.append(part + \".\")\n",
    "            current_sentence = parts[-1]\n",
    "    \n",
    "    if current_sentence:\n",
    "        merged.append(current_sentence)\n",
    "        \n",
    "    return merged\n",
    "\n",
    "sentences = merge_lines(text.split(\"\\n\"))\n",
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar embeddings e armazenar no banco de dados vetorial\n",
    "\n",
    "Vamos usar o ChromaDB e manter o banco de dados em memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404/404 [00:23<00:00, 17.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"artigo_exemplo\")\n",
    "\n",
    "for idx, sentence in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "    collection.add(\n",
    "        documents=[sentence],\n",
    "        ids=[f\"sentence_{idx}\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG?\n",
      "\n",
      "Document: The power of noise: Redefining retrieval for RAG systems.\n",
      "Distance: 0.7560287714004517\n",
      "\n",
      "Document: RAG systems trained end-to-end (e.g.\n",
      "Distance: 0.7731610536575317\n",
      "\n",
      "Document: RAG pipelines are made up of two disparate components: a retriever, which identifies documents relevant to a query from a given corpus, and a reader, which is typically an LLM prompted with a query, the text of the retrieved documents, and instructions to use this context to generate its response.\n",
      "Distance: 0.9335125088691711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Teste simples de recuperação\n",
    "\n",
    "# Essa query só é usada nesta célula, uma nova query para o pipeline completo será definida mais para frente\n",
    "query = \"What is RAG?\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3, \n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for idx, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Distance: {distance}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de geração\n",
    "\n",
    "Vamos usar a API da OpenAI, mas pode ser trocada por Gemini ou outra gratuita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A query a ser usada daqui para frente\n",
    "\n",
    "query = \"Que modelos de LLMs são avaliados e qual é o principal resultado do artigo?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Usando exclusivamente as informações contidas na seção Base de Dados, responda a pergunta contida na seção Pergunta do Usuário. Se não há informações suficientes para responder a pergunta, responda com algo indicando que não há informações suficientes. Responda na mesma língua da pergunta. Não mencione a Base de Dados na resposta.\\n\\nAo responder, siga estas instruções:\\n- Seja objetivo e direto.\\n- Seja formal e profissional.\\n- Se a pergunta não tiver relação com o artigo, não responda.\\n\\n# Pergunta do Usuário\\n{query}\\n\\n# Base de Dados\\n{chunks}\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ler os templates para os prompts\n",
    "# Notar que os prompts estão em português, o que não é ideal dado que o artigo sendo processado é em inglês\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(\"prompt_template.yml\", \"r\") as file:\n",
    "    prompts = yaml.safe_load(file)\n",
    "\n",
    "system_prompt = prompts[\"System_Prompt\"]\n",
    "prompt_template = prompts[\"Prompt\"]\n",
    "prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# Carregar variáveis de ambiente (devem estar no arquivo .env)\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def get_completion(prompt, system_prompt, model=\"gpt-4o-mini\", json_format=False):\n",
    "    \"\"\"\n",
    "    Obtém uma resposta do modelo de linguagem usando a API OpenAI.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): O prompt principal a ser enviado ao modelo\n",
    "        system_prompt (str): O prompt de sistema que define o comportamento do modelo\n",
    "        model (str, optional): Nome do modelo a ser usado. Padrão é \"gpt-4o-mini\"\n",
    "        json_format (bool, optional): Se True, força a resposta em formato JSON. Padrão é False\n",
    "\n",
    "    Returns:\n",
    "        str: A resposta gerada pelo modelo\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=500,\n",
    "        response_format={\"type\": \"json_object\"} if json_format else None\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansão de query\n",
    "\n",
    "Vamos processar a query para facilitar a tarefa de recuperação. Aqui fazemos duas coisas:\n",
    "1. Identificamos se a query é composta (i.e. se possui mais de uma pergunta)\n",
    "2. Extraímos termos relevantes para cada pergunta identificada\n",
    "3. Geramos uma lista de respostas fictícias para cada pergunta identificada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LLM, model, models', 'results, outcomes, findings, contributions']\n",
      "['The article evaluates several LLM models, including BERT, GPT-3, and T5, highlighting their performance on various benchmarks.', 'The main result of the article indicates that the proposed model outperforms existing LLMs by achieving a 15% increase in accuracy on the test dataset.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "prompt = prompts[\"Prompt_Expansao\"].format(query=query)\n",
    "\n",
    "response = get_completion(prompt, \"\", json_format=True)\n",
    "\n",
    "response_json = json.loads(response)\n",
    "queries = response_json['termos']\n",
    "respostas = response_json['respostas_ficticias']\n",
    "print(queries)\n",
    "print(respostas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar como queries tanto os termos como as respostas fictícias\n",
    "\n",
    "queries = queries + respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando exclusivamente as informações contidas na seção Base de Dados, responda a pergunta contida na seção Pergunta do Usuário. Se não há informações suficientes para responder a pergunta, responda com algo indicando que não há informações suficientes. Responda na mesma língua da pergunta. Não mencione a Base de Dados na resposta.\n",
      "\n",
      "Ao responder, siga estas instruções:\n",
      "- Seja objetivo e direto.\n",
      "- Seja formal e profissional.\n",
      "- Se a pergunta não tiver relação com o artigo, não responda.\n",
      "\n",
      "# Pergunta do Usuário\n",
      "Que modelos de LLMs são avaliados e qual é o principal resultado do artigo?\n",
      "\n",
      "# Base de Dados\n",
      "3 Experiment setup We conduct our experiments with two instruction-tuned LLMs: LLaMA (Llama-2-7b-chat) ( 18) and Mistral (Mistral-7B-Instruct-v0.3) ( 19).\n",
      "\n",
      "This difference between LLMs is likely due to LLaMA’s shorter context window.\n",
      "\n",
      "We ran LLM inference on NVIDIA GPUs of varying models (NVIDIA Titan Xp or X Pascal series, or NVIDIA A40).\n",
      "\n",
      "Llama: Open and efficient foundation language models, 2023.\n",
      "\n",
      "A similar pattern of QA performance was observed when using the same LLM as (7) (Appendix A.7).\n",
      "\n",
      "This is relatively unexplored in the literature, as most have investigated how well LLMs can utilize the context and ignore non-gold documents ( 16;26;29;30).\n",
      "\n",
      "Retrieval results were saved to files and were inserted into the prompt (Figure 1) for the LLM during the reader portion of the experiments.\n",
      "\n",
      "In attributed QA, an LLM is also required to explicitly cite (e.g., by document ID) one or more of the documents used.\n",
      "\n",
      "Knowledge editing for large language models: A survey, 2024.\n",
      "\n",
      "A temperature of 1 and top p of 0.95 were used for generation with both LLMs.\n",
      "\n",
      "Future work should test the generality of these findings in other settings.\n",
      "\n",
      "Retrieval results were saved to files and were inserted into the prompt (Figure 1) for the LLM during the reader portion of the experiments.\n",
      "\n",
      "In this work, we study the contributions of retrieval to downstream performance.1.\n",
      "\n",
      "Figure 12 shows the results of these two experiments.\n",
      "\n",
      "at higher k) results in more extraneous, or unnecessary, citations.\n",
      "\n",
      "We make four contributions: (1) We show how both QA performance and citation metrics vary with more retrieved documents, adding new data to a small literature on attributed QA with RAG.\n",
      "\n",
      "Retrieval models.\n",
      "\n",
      "Interestingly though, citation scores improve for farther neighbors.\n",
      "\n",
      "We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines.\n",
      "\n",
      "Citation measures for other datasets and models are in A.4.\n",
      "\n",
      "2https://github.com/intel/ScalableVectorSearch 3 4 Results We first analyze how many retrieved documents should be included in the LLM context window to maximize correctness on the selected QA tasks.\n",
      "\n",
      "Toward Optimal Search and Retrieval for RAG Alexandria Leto Department of Computer Science University of Colorado Boulder alex.leto@colorado.eduCecilia Aguerrebere Intel Labs cecilia.aguerrebere@intel.com Ishwar Bhati Intel Labs ishwar.s.bhati@intel.comTed Willke Intel Labs ted.willke@intel.com Mariano Tepper Intel Labs mariano.tepper@intel.comVy Ai Vo Intel Labs vy.vo@intel.com Abstract Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs).\n",
      "\n",
      "3 Experiment setup We conduct our experiments with two instruction-tuned LLMs: LLaMA (Llama-2-7b-chat) ( 18) and Mistral (Mistral-7B-Instruct-v0.3) ( 19).\n",
      "\n",
      "For this purpose, we evaluate pipelines with separately trained retriever and LLM components, as training retrieval- augmented models end-to-end is both more resource-intensive and obfuscates the contribution of the retriever itself.\n",
      "\n",
      "This is relatively unexplored in the literature, as most have investigated how well LLMs can utilize the context and ignore non-gold documents ( 16;26;29;30).\n",
      "\n",
      "1 Introduction Retrieval-augmented generation (RAG) ( 1) is gaining popularity due to its ability to address some of the challenges with using Large Language Models (LLMs), including hallucinations ( 2) and outdated training data ( 1;3).\n",
      "\n",
      "3.1 Retrieval We chose to evaluate two high-performing, open-source dense retrieval models.\n",
      "\n",
      "Since citation precision measures how many of the cited documents are required for each statement, this suggests that showing the LLM more documents (i.e.\n",
      "\n",
      "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\n",
      "\n",
      "Benchmarking Large Language Models in Retrieval- Augmented Generation.\n",
      "\n",
      "A similar pattern of QA performance was observed when using the same LLM as (7) (Appendix A.7).\n",
      "\n",
      "3 Experiment setup We conduct our experiments with two instruction-tuned LLMs: LLaMA (Llama-2-7b-chat) ( 18) and Mistral (Mistral-7B-Instruct-v0.3) ( 19).\n",
      "\n",
      "This difference between LLMs is likely due to LLaMA’s shorter context window.\n",
      "\n",
      "Retrieval results were saved to files and were inserted into the prompt (Figure 1) for the LLM during the reader portion of the experiments.\n",
      "\n",
      "This is relatively unexplored in the literature, as most have investigated how well LLMs can utilize the context and ignore non-gold documents ( 16;26;29;30).\n",
      "\n",
      "Since citation precision measures how many of the cited documents are required for each statement, this suggests that showing the LLM more documents (i.e.\n",
      "\n",
      "A.4 Additional varied number of neighbors results Correctness results for all three datasets with LLaMA are in Figure 8.\n",
      "\n",
      "Figure 11: The per-query relationship between the number of gold documents included in the prompt and the QA accuracy achieved with LLaMA on NQ.\n",
      "\n",
      "Figure 9: The per-query relationship between the number of gold documents included in the prompt and the QA accuracy achieved with LLaMA on ASQA.\n",
      "\n",
      "We ran LLM inference on NVIDIA GPUs of varying models (NVIDIA Titan Xp or X Pascal series, or NVIDIA A40).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recuperação e construção do prompt\n",
    "all_docs = []\n",
    "for query_ in queries:\n",
    "    docs = collection.query(\n",
    "        query_texts=[query_],\n",
    "        n_results=10\n",
    "    )\n",
    "    all_docs.extend(docs['documents'][0])\n",
    "\n",
    "\n",
    "formatted_chunks = \"\\n\".join([f\"{chunk}\\n\" for chunk in all_docs])\n",
    "\n",
    "prompt = prompt_template.format(chunks=formatted_chunks, query=query)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que modelos de LLMs são avaliados e qual é o principal resultado do artigo?\n",
      "Os modelos de LLMs avaliados no artigo são LLaMA (Llama-2-7b-chat) e Mistral (Mistral-7B-Instruct-v0.3). O principal resultado do artigo é que a performance de QA e as métricas de citação variam com o número de documentos recuperados, revelando insights úteis para o desenvolvimento de pipelines de RAG de alto desempenho.\n"
     ]
    }
   ],
   "source": [
    "# Geração da resposta\n",
    "\n",
    "response = get_completion(prompt, system_prompt)\n",
    "print(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_ufpel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
